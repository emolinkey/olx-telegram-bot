import os
import asyncio
import cloudscraper
from bs4 import BeautifulSoup
from aiogram import Bot
from flask import Flask
import threading
import random
import sys
import json

# --- –ö–û–ù–§–ò–ì ---
TOKEN = "8346602599:AAFj8lQ_cfMwBXIfOSl7SbA9J7qixcpaO68"
CHAT_ID = "908015235"
OLX_URL = "https://www.olx.pl/elektronika/komputery/podzespoly-i-czesci/q-pami%C4%99%C4%87-ram-ddr4-8gb/?search%5Bfilter_float_price%3Afrom%5D=100&search%5Bfilter_float_price%3Ato%5D=250&search%5Border%5D=created_at%3Adesc"

# --- –ü–†–û–ö–°–ò ---
PROXY = "http://nyntgqyu:2c5wo0xukywv@64.137.96.74:6641"
PROXIES = {
    "http": PROXY,
    "https": PROXY
}

# --- –í–ï–ë-–°–ï–†–í–ï–† ---
app = Flask('')

@app.route('/')
def home():
    return "SYSTEM ONLINE"

def run_flask():
    port = int(os.environ.get("PORT", 10000))
    app.run(host='0.0.0.0', port=port)

# --- –ú–û–ù–ò–¢–û–† ---
class OLXProMonitor:
    def __init__(self):
        self.bot = Bot(token=TOKEN)
        self.seen_ads = set()
        self.scraper = None

    def create_scraper(self):
        self.scraper = cloudscraper.create_scraper(
            browser={
                'browser': 'chrome',
                'platform': 'windows',
                'desktop': True
            },
            delay=5
        )
        self.scraper.proxies = PROXIES
        self.scraper.headers.update({
            "Accept-Language": "pl-PL,pl;q=0.9,en-US;q=0.8,en;q=0.7",
            "Referer": "https://www.google.com/",
        })
        print("‚úÖ Scraper —Å–æ–∑–¥–∞–Ω (Chrome –∏–º–∏—Ç–∞—Ü–∏—è + –ø—Ä–æ–∫—Å–∏)")
        sys.stdout.flush()

    def fetch_ads_sync(self):
        try:
            if not self.scraper:
                self.create_scraper()

            # –°–Ω–∞—á–∞–ª–∞ –∑–∞—Ö–æ–¥–∏–º –Ω–∞ –≥–ª–∞–≤–Ω—É—é ‚Äî –ø–æ–ª—É—á–∞–µ–º cookies
            print("üåê –ó–∞—Ö–æ–∂—É –Ω–∞ –≥–ª–∞–≤–Ω—É—é OLX...")
            sys.stdout.flush()

            try:
                self.scraper.get("https://www.olx.pl/", timeout=30)
                import time
                time.sleep(random.uniform(2, 4))
            except Exception as e:
                print(f"‚ö†Ô∏è –ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞: {e}")

            # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –æ–±—ä—è–≤–ª–µ–Ω–∏—è
            print("üîç –ó–∞–ø—Ä–∞—à–∏–≤–∞—é –æ–±—ä—è–≤–ª–µ–Ω–∏—è...")
            sys.stdout.flush()

            r = self.scraper.get(OLX_URL, timeout=30)
            print(f"üì° –°—Ç–∞—Ç—É—Å: {r.status_code}")
            sys.stdout.flush()

            if r.status_code == 403:
                print("‚ùå 403 ‚Äî –ø–µ—Ä–µ—Å–æ–∑–¥–∞—é scraper...")
                self.scraper = None
                return []

            if r.status_code != 200:
                print(f"‚ùå –°—Ç–∞—Ç—É—Å: {r.status_code}")
                return []

            soup = BeautifulSoup(r.text, "html.parser")
            ads = []

            # === –ú–ï–¢–û–î 1: __NEXT_DATA__ ===
            next_data = soup.find("script", {"id": "__NEXT_DATA__"})
            if next_data and next_data.string:
                try:
                    data = json.loads(next_data.string)
                    ads = self.parse_next_data(data)
                    if ads:
                        print(f"‚úÖ [NEXT_DATA] –ù–∞–π–¥–µ–Ω–æ: {len(ads)}")
                        return ads
                except Exception as e:
                    print(f"‚ö†Ô∏è [NEXT_DATA] –û—à–∏–±–∫–∞: {e}")

            # === –ú–ï–¢–û–î 2: JSON –≤ script —Ç–µ–≥–∞—Ö ===
            for script in soup.find_all("script", {"type": "application/json"}):
                try:
                    if script.string:
                        data = json.loads(script.string)
                        found = self.deep_search(data)
                        if found:
                            print(f"‚úÖ [JSON] –ù–∞–π–¥–µ–Ω–æ: {len(found)}")
                            return found
                except:
                    continue

            # === –ú–ï–¢–û–î 3: data-cy –∫–∞—Ä—Ç–æ—á–∫–∏ ===
            cards = soup.find_all("div", {"data-cy": "l-card"})
            print(f"üìã [HTML] –ö–∞—Ä—Ç–æ—á–µ–∫ data-cy: {len(cards)}")

            for card in cards:
                link = card.find("a", href=True)
                if link and '/d/oferta/' in link.get('href', ''):
                    href = link['href']
                    url = href if href.startswith("http") else "https://www.olx.pl" + href
                    clean = url.split("#")[0].split("?")[0].rstrip('/')

                    title_el = card.find("h6") or card.find("h4") or card.find("h3")
                    title = title_el.get_text(strip=True) if title_el else "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"

                    price_el = card.find("p", {"data-testid": "ad-price"})
                    price = price_el.get_text(strip=True) if price_el else "?"

                    ads.append({
                        "title": title,
                        "url": clean,
                        "price": price
                    })

            if ads:
                print(f"‚úÖ [HTML cards] –ù–∞–π–¥–µ–Ω–æ: {len(ads)}")
                return ads

            # === –ú–ï–¢–û–î 4: –≤—Å–µ —Å—Å—ã–ª–∫–∏ /d/oferta/ ===
            seen = set()
            for a in soup.find_all("a", href=True):
                href = a['href']
                if '/d/oferta/' in href:
                    url = href if href.startswith("http") else "https://www.olx.pl" + href
                    clean = url.split("#")[0].split("?")[0].rstrip('/')
                    if clean not in seen:
                        seen.add(clean)
                        text = a.get_text(strip=True)[:100]
                        ads.append({
                            "title": text if text else "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è",
                            "url": clean,
                            "price": "?"
                        })

            if ads:
                print(f"‚úÖ [LINKS] –ù–∞–π–¥–µ–Ω–æ: {len(ads)}")
            else:
                # –î–µ–±–∞–≥ ‚Äî –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–∞—á–∞–ª–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                text_preview = soup.get_text()[:300].strip()
                print(f"‚ö†Ô∏è –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ. –¢–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã:\n{text_preview}")

            return ads

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            sys.stdout.flush()
            self.scraper = None
            return []

    def parse_next_data(self, data):
        ads = []
        try:
            props = data.get("props", {}).get("pageProps", {})

            items = []

            # –ü—É—Ç—å 1
            listing = props.get("listing", {})
            if isinstance(listing, dict):
                inner = listing.get("listing", {})
                if isinstance(inner, dict):
                    items = inner.get("ads", [])

            # –ü—É—Ç—å 2
            if not items:
                items = props.get("ads", [])

            # –ü—É—Ç—å 3
            if not items:
                d = props.get("data", {})
                if isinstance(d, dict):
                    items = d.get("ads", [])

            # –ü—É—Ç—å 4 ‚Äî –≥–ª—É–±–æ–∫–∏–π –ø–æ–∏—Å–∫
            if not items:
                return self.deep_search(data)

            for item in items:
                url = item.get("url", "")
                if not url:
                    continue
                if not url.startswith("http"):
                    url = "https://www.olx.pl" + url
                clean = url.split("#")[0].split("?")[0].rstrip('/')

                title = item.get("title", "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è")

                price = "?"
                pd = item.get("price", {})
                if isinstance(pd, dict):
                    price = pd.get("displayValue",
                            pd.get("regularPrice", {}).get("displayValue", "?"))
                elif pd:
                    price = str(pd)

                ads.append({
                    "title": title,
                    "url": clean,
                    "price": price
                })

        except Exception as e:
            print(f"‚ö†Ô∏è parse_next_data: {e}")

        return ads

    def deep_search(self, data, results=None):
        if results is None:
            results = []

        if isinstance(data, dict):
            url = data.get("url", "")
            title = data.get("title", "")
            if url and title and "/d/oferta/" in str(url):
                if not url.startswith("http"):
                    url = "https://www.olx.pl" + url
                clean = url.split("#")[0].split("?")[0].rstrip('/')

                price = "?"
                p = data.get("price", {})
                if isinstance(p, dict):
                    price = p.get("displayValue", "?")

                existing_urls = [r["url"] for r in results]
                if clean not in existing_urls:
                    results.append({
                        "title": title,
                        "url": clean,
                        "price": price
                    })

            for v in data.values():
                self.deep_search(v, results)

        elif isinstance(data, list):
            for item in data:
                self.deep_search(item, results)

        return results

    def format_message(self, ad):
        return (
            f"üÜï –ù–û–í–û–ï –û–ë–™–Ø–í–õ–ï–ù–ò–ï!\n\n"
            f"üì¶ {ad['title']}\n"
            f"üí∞ {ad['price']}\n"
            f"üîó {ad['url']}"
        )

    async def run(self):
        threading.Thread(target=run_flask, daemon=True).start()
        print("=" * 50)
        print("üöÄ –ë–û–¢ –°–¢–ê–†–¢–û–í–ê–õ")
        print("üîí –î–≤–∏–∂–æ–∫: cloudscraper (Chrome)")
        print("üåê –ü—Ä–æ–∫—Å–∏: –ò—Å–ø–∞–Ω–∏—è")
        print("=" * 50)
        sys.stdout.flush()

        try:
            await self.bot.send_message(
                CHAT_ID,
                "‚úÖ –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∑–∞–ø—É—â–µ–Ω!\n"
                "üîí –î–≤–∏–∂–æ–∫: cloudscraper\n"
                "üåê –ü—Ä–æ–∫—Å–∏: –ò—Å–ø–∞–Ω–∏—è\n"
                "üîÑ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞–∂–¥—ã–µ 5-7 –º–∏–Ω—É—Ç"
            )
        except Exception as e:
            print(f"‚ùå Telegram –æ—à–∏–±–∫–∞: {e}")

        fail_count = 0

        while True:
            try:
                # cloudscraper —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π ‚Äî –∑–∞–ø—É—Å–∫–∞–µ–º –≤ –ø–æ—Ç–æ–∫–µ
                loop = asyncio.get_event_loop()
                ads = await loop.run_in_executor(None, self.fetch_ads_sync)

                print(f"üìä –í—Å–µ–≥–æ: {len(ads)}")
                sys.stdout.flush()

                if ads:
                    fail_count = 0

                    if not self.seen_ads:
                        for ad in ads:
                            self.seen_ads.add(ad['url'])
                        await self.bot.send_message(
                            CHAT_ID,
                            f"üì° –ë–∞–∑–∞ —Å–æ–±—Ä–∞–Ω–∞: {len(ads)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π\n"
                            f"üîç –û—Ç—Å–ª–µ–∂–∏–≤–∞—é –Ω–æ–≤—ã–µ..."
                        )
                        print(f"‚úÖ –ë–∞–∑–∞: {len(ads)} —à—Ç")
                    else:
                        new_count = 0
                        for ad in ads:
                            if ad['url'] not in self.seen_ads:
                                self.seen_ads.add(ad['url'])
                                new_count += 1
                                try:
                                    await self.bot.send_message(
                                        CHAT_ID,
                                        self.format_message(ad)
                                    )
                                    await asyncio.sleep(1)
                                except Exception as e:
                                    print(f"‚ùå –û—Ç–ø—Ä–∞–≤–∫–∞: {e}")

                        if new_count:
                            print(f"üÜï –ù–æ–≤—ã—Ö: {new_count}")
                        else:
                            print("‚ÑπÔ∏è –ù–æ–≤—ã—Ö –Ω–µ—Ç")
                else:
                    fail_count += 1
                    print(f"‚ö†Ô∏è –ü—É—Å—Ç–æ ({fail_count}/5)")

                    if fail_count >= 5:
                        fail_count = 0
                        self.scraper = None
                        try:
                            await self.bot.send_message(
                                CHAT_ID,
                                "‚ö†Ô∏è 5 –Ω–µ—É–¥–∞—á –ø–æ–¥—Ä—è–¥.\n"
                                "–ü–µ—Ä–µ—Å–æ–∑–¥–∞—é scraper..."
                            )
                        except:
                            pass

                sys.stdout.flush()

            except Exception as e:
                print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
                sys.stdout.flush()
                self.scraper = None

            delay = random.randint(300, 420)
            print(f"‚è≥ –°–ª–µ–¥—É—é—â–∞—è —á–µ—Ä–µ–∑ {delay // 60}–º {delay % 60}—Å")
            sys.stdout.flush()
            await asyncio.sleep(delay)


if __name__ == "__main__":
    monitor = OLXProMonitor()
    asyncio.run(monitor.run())
